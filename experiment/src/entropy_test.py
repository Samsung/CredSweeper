#!/usr/bin/env python3
"""
The script is used in experiment to get statistical distribution of shanon entropy
of a line which was obtained with an encoding (base64, base32, etc.) from random generated bytes.
The result format is:
# size of encoded string: (mean of entropy, standard deviation)
"""

import base64
import random
import signal
import statistics
import threading
import time
from multiprocessing import Pool
from typing import Tuple, Dict

import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import curve_fit

from credsweeper.common.constants import Chars
# from credsweeper.filters import ValueEntropyBase36Check
from credsweeper.utils import Util

random_data: str
ITERATIONS = 1000


def pool_initializer() -> None:
    signal.signal(signal.SIGINT, signal.SIG_IGN)


def evaluate_avg(_args: Tuple[int, float, float]) -> Tuple[float, float]:
    min_avg = _args[1]
    min_dvt = _args[2]
    size = _args[0]
    entropies = []
    for x in range(ITERATIONS):
        offset = x * size
        entropy = Util.get_shannon_entropy(random_data[offset:offset + size], Chars.BASE64_CHARS.value)
        # entropy = Util.get_shannon_entropy(random_data[offset:offset + size], Chars.BASE36_CHARS.value)
        # entropy = Util.get_shannon_entropy(random_data[offset:offset + size], Chars.BASE32_CHARS.value)
        entropies.append(entropy)
    avg = statistics.mean(entropies)
    dvt = statistics.stdev(entropies, avg)
    if avg < min_avg:
        min_avg = avg
        min_dvt = dvt
    return min_avg, min_dvt


def generate(start, end) -> Dict[int, Tuple[float, float]]:
    stats: Dict[int, Tuple[float, float]] = {}  # type: ignore
    sizes = [x for x in range(start, end)]
    global random_data
    try:
        for n in range(1000):
            start_time = time.time()
            rand_bytes = random.randbytes(int(8 * ITERATIONS * max(sizes) / 5))
            random_data = base64.b64encode(rand_bytes).decode('ascii')
            # random_data = ''.join(
            #     [random.choice(string.digits + string.ascii_lowercase) for _ in range(ITERATIONS * max(sizes))])
            _args = [(i, stats[i][0] if i in stats else 9.9, stats[i][1] if i in stats else 0.0) for i in sizes]
            with Pool(processes=min(15, len(_args)), initializer=pool_initializer) as pool:
                for _size, _res in zip(sizes, pool.map(evaluate_avg, _args)):
                    with threading.Lock():
                        stats[_size] = _res
            print(f"done {n} in {time.time() - start_time}", flush=True)
    except KeyboardInterrupt as exc:
        print(exc)
    finally:
        print("===========================================================", flush=True)
    for k, v in stats.items():
        print(f"{k}: {v},", flush=True)
    return stats


def log_model(x, k4, k3, k2, k1, k0):
    return k4 * np.log2(x)**4 + k3 * np.log2(x)**3 + k2 * np.log2(x)**2 + k1 * np.log2(x) + k0


def solve(data: dict[int, Tuple[float, float]]):
    d_list = list((x, y) for x, y in data.items())
    d_list.sort(key=lambda x: (int(x[0])))

    plt.figure()
    x = [int(i[0]) for i in d_list]
    y = [i[1][0] for i in d_list]
    y_min = [i[1][0] - i[1][1] for i in d_list]
    y_max = [i[1][0] + i[1][1] for i in d_list]
    plt.plot(x, y, 'r-', lw=2, label='ent')
    plt.plot(x, y_min, 'r:', lw=1, label='min')
    plt.plot(x, y_max, 'r:', lw=1, label='max')

    _y = np.array(y_min)
    _x = np.array(x)

    params, covariance = curve_fit(log_model, _x, _y)
    print(params)
    k4, k3, k2, k1, k0 = params
    plt.plot(x, log_model(x, k4, k3, k2, k1, k0), 'b--', label='fit')

    plt.grid(True)
    plt.show()


if __name__ == "__main__":
    data_file = "base64entr_12_1200.json"  # [0.00147696 -0.03688593  0.24484864  0.31841099  0.39320007]
    if not (_data := Util.json_load(data_file)):
        _data = generate(12, 1200)
        Util.json_dump(_data, data_file)
    solve(_data)
