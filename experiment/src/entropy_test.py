#!/usr/bin/env python3
"""
The script is used in experiment to get statistical distribution of shanon entropy
of a line which was obtained with an encoding (base64, base32, etc.) from random generated bytes.
The result format is:
# size of encoded string: (mean of entropy, standard deviation)
"""

import base64
import math
import random
import signal
import statistics
import sys
import threading
import time
from multiprocessing import Pool
from typing import Tuple, Dict

import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import curve_fit

from credsweeper.common.constants import Chars
# from credsweeper.filters import ValueEntropyBase36Check
from credsweeper.utils import Util

random_data: bytes
ITERATIONS = 100


def pool_initializer() -> None:
    signal.signal(signal.SIGINT, signal.SIG_IGN)


def byte_entropy(data: bytes):
    data_len = len(data)
    entropy = 0.
    cells = [int(0)] * 256
    for x in data:
        cells[x] += 1
    left = 0.
    step = 256.0 / data_len
    right = left + step
    while left < 256:
        cell_sum = 0
        i = int(left)
        r = int(right)
        while i < r and i < 256:
            cell_sum += cells[i]
            i += 1
        p_x = float(cell_sum) / data_len
        if p_x > 0:
            entropy += -p_x * math.log2(p_x)
        left = right
        right += step
    return entropy


def evaluate_avg(_args: Tuple[int, float, float]) -> Tuple[float, float]:
    min_avg = _args[1]
    min_dvt = _args[2]
    size = _args[0]
    entropies = []
    for x in range(ITERATIONS):
        offset = x * size
        entropy = byte_entropy(random_data[offset:offset + size])
        # entropy = Util.get_shannon_entropy(random_data[offset:offset + size], Chars.BASE64_CHARS.value)
        # entropy = Util.get_shannon_entropy(random_data[offset:offset + size], Chars.BASE36_CHARS.value)
        # entropy = Util.get_shannon_entropy(random_data[offset:offset + size], Chars.BASE32_CHARS.value)
        entropies.append(entropy)
    avg = statistics.mean(entropies)
    dvt = statistics.stdev(entropies, avg)
    if avg < min_avg:
        min_avg = avg
        min_dvt = dvt
    return min_avg, min_dvt


def generate(start, end) -> Dict[int, Tuple[float, float]]:
    stats: Dict[int, Tuple[float, float]] = {}  # type: ignore
    sizes = [x for x in range(start, end)]
    global random_data
    try:
        for n in range(1000):
            start_time = time.time()
            random_data = random.randbytes(ITERATIONS * max(sizes))
            # random_data = ''.join(
            #     [random.choice(string.digits + string.ascii_lowercase) for _ in range(ITERATIONS * max(sizes))])
            _args = [(i, stats[i][0] if i in stats else 9.9, stats[i][1] if i in stats else 0.0) for i in sizes]
            with Pool(processes=min(15, len(_args)), initializer=pool_initializer) as pool:
                for _size, _res in zip(sizes, pool.map(evaluate_avg, _args)):
                    with threading.Lock():
                        stats[_size] = _res
            print(f"done {n} in {time.time() - start_time}", flush=True)
    except KeyboardInterrupt as exc:
        print(exc)
    finally:
        print("===========================================================", flush=True)
    for k, v in stats.items():
        print(f"{k}: {v},", flush=True)
    return stats


def log_model(x, k1, k0):
    return k1 * np.log2(x) + k0


def solve(data: dict[int, Tuple[float, float]]):
    d_list = list((x, y) for x, y in data.items())
    d_list.sort(key=lambda x: (int(x[0])))

    plt.figure()
    x = [int(i[0]) for i in d_list]
    y = [i[1][0] for i in d_list]
    y_min = [i[1][0] - i[1][1] for i in d_list]
    y_max = [i[1][0] + i[1][1] for i in d_list]
    plt.plot(x, y, 'r-', lw=2, label='ent')
    plt.plot(x, y_min, 'r:', lw=1, label='min')
    plt.plot(x, y_max, 'r:', lw=1, label='max')

    _y = np.array(y_min)
    _x = np.array(x)

    params, covariance = curve_fit(log_model, _x, _y)
    print(params)
    k1, k0 = params
    plt.plot(x, log_model(x, k1, k0), 'b--', label='fit')

    plt.grid(True)
    plt.show()

from scipy.stats import entropy
import numpy as np

def calculate_shannon_entropy(byte_sequence):
    byte_counts = np.bincount(byte_sequence, minlength=256)
    # Normalize the counts to get the probabilities
    probabilities = byte_counts / np.sum(byte_counts)
    # Calculate the entropy
    return entropy(probabilities, base=2)

if __name__ == "__main__":
    # data = [0]*200
    # for n in range(len(data)):
    #     data[n]=n>>2
    # print(byte_entropy(data))
    # print(calculate_shannon_entropy(data))
    # sys.exit(0)
    # data_file = "base64entr_12_1200.json"  # [0.00147696 -0.03688593  0.24484864  0.31841099  0.39320007]
    start, end = 63, 130
    data_file = f"bytes_{start}_{end}.json"  #[ 1.01660278 -1.03603384]
    if not (_data := Util.json_load(data_file)):
        _data = generate(start, end)
        Util.json_dump(_data, data_file)
    solve(_data)
